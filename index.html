<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/pygment_trac.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>Machine Learning - Model For Predicting Exercise Quality by ajammala</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1>Machine Learning - Model For Predicting Exercise Quality</h1>
        <h2>Practical Machine Learning Project</h2>
        <a href="https://github.com/ajammala/MachineLearning" class="button"><small>View project on</small>GitHub</a>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <h3>
<a name="model-for-predicting-exercise-quality" class="anchor" href="#model-for-predicting-exercise-quality"><span class="octicon octicon-link"></span></a>Model For Predicting Exercise Quality</h3>

<h4>
<a name="10-summary" class="anchor" href="#10-summary"><span class="octicon octicon-link"></span></a>1.0 Summary:</h4>

<p>In a study, 6 test subjects were asked to perform barbell lifts correctly and incorrectly in 5 different ways while wearing accelerometers on the belt, forearms, arms and the dumbells. The data from accelerometers was recorded. The goal of this project is to build a supervised machine learning model that uses the recorded data to predict the manner in which the exercise was performed on a test data set. </p>

<h4>
<a name="20-exploratory-data-analysis" class="anchor" href="#20-exploratory-data-analysis"><span class="octicon octicon-link"></span></a>2.0 Exploratory Data Analysis</h4>

<h5>
<a name="21-downloading-the-data" class="anchor" href="#21-downloading-the-data"><span class="octicon octicon-link"></span></a>2.1 Downloading The Data</h5>

<pre><code>if (!file.exists("./pml-training.csv")) {
    download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
        destfile = "./pml-training.csv")
}
if (!file.exists("./pml-testing.csv")) {
    download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
        destfile = "./pml-testing.csv")
}
</code></pre>

<p>Reading the data into a data frame. </p>

<pre><code>input_data &lt;- read.csv("./pml-training.csv")
test_data &lt;- read.csv("./pml-testing.csv")
str(input_data)
</code></pre>

<h5>
<a name="22-working-with-missing-values" class="anchor" href="#22-working-with-missing-values"><span class="octicon octicon-link"></span></a>2.2 Working With Missing Values</h5>

<p>Now that we have the data in a data frame, it is time to explore the columns in the data set. We know that the data set has 19622 records with 160 columns each. 
We first try to look at the missing values in the data set.</p>

<pre><code>df &lt;- colSums(is.na(input_data))
plot(df, xlab = "Variable Index", ylab="Number of NAs", type='l')
</code></pre>

<p><img src="https://raw.githubusercontent.com/ajammala/MachineLearning/master/untitled.png" alt="">
The plot shows us that a lot of fields contain a significant number of NAs (close to 19000 NAs out of 19622 observations). We can remove these values safely because they do not affect the outcome of the experiment.  </p>

<pre><code>input_data &lt;- read.csv("./pml-training.csv", na.strings=c("NA",""))
input_data &lt;- input_data[, - which(as.numeric(colSums(is.na(input_data))) &gt; 19000)]
</code></pre>

<p>The first 7 columns in the data set are <code>X</code>, <code>user_name</code>, <code>raw_timestamp_part_1</code>, <code>raw_timestamp_part_2</code>, <code>cvtd_timestamp</code>, <code>new_window</code> and <code>num_window</code>. These are static values and do not directly impact the outcome of the experiment. They can be safely removed from the data set.</p>

<pre><code>input_data &lt;- input_data[, -c(1:7)]
dim(input_data)
names(input_data)
</code></pre>

<pre><code>##  [1] "roll_belt"            "pitch_belt"           "yaw_belt"            
##  [4] "total_accel_belt"     "gyros_belt_x"         "gyros_belt_y"        
##  [7] "gyros_belt_z"         "accel_belt_x"         "accel_belt_y"        
## [10] "accel_belt_z"         "magnet_belt_x"        "magnet_belt_y"       
## [13] "magnet_belt_z"        "roll_arm"             "pitch_arm"           
## [16] "yaw_arm"              "total_accel_arm"      "gyros_arm_x"         
## [19] "gyros_arm_y"          "gyros_arm_z"          "accel_arm_x"         
## [22] "accel_arm_y"          "accel_arm_z"          "magnet_arm_x"        
## [25] "magnet_arm_y"         "magnet_arm_z"         "roll_dumbbell"       
## [28] "pitch_dumbbell"       "yaw_dumbbell"         "total_accel_dumbbell"
## [31] "gyros_dumbbell_x"     "gyros_dumbbell_y"     "gyros_dumbbell_z"    
## [34] "accel_dumbbell_x"     "accel_dumbbell_y"     "accel_dumbbell_z"    
## [37] "magnet_dumbbell_x"    "magnet_dumbbell_y"    "magnet_dumbbell_z"   
## [40] "roll_forearm"         "pitch_forearm"        "yaw_forearm"         
## [43] "total_accel_forearm"  "gyros_forearm_x"      "gyros_forearm_y"     
## [46] "gyros_forearm_z"      "accel_forearm_x"      "accel_forearm_y"     
## [49] "accel_forearm_z"      "magnet_forearm_x"     "magnet_forearm_y"    
## [52] "magnet_forearm_z"     "classe"
</code></pre>

<h4>
<a name="30-feature-selection" class="anchor" href="#30-feature-selection"><span class="octicon octicon-link"></span></a>3.0 Feature Selection</h4>

<p>The next step is to select the predictor variables to be used in the model.</p>

<h5>
<a name="31-corelated-predictors" class="anchor" href="#31-corelated-predictors"><span class="octicon octicon-link"></span></a>3.1 Corelated Predictors</h5>

<p>We now check for Corelated predictors in the data set. If two variables are highly correlated they will impart nearly exactly the same information to the regression model. Including both variables will result in a weak model by infusing the model with noise. </p>

<pre><code>library(caret)
set.seed(1016)
in_train &lt;- createDataPartition(input_data$classe, p=0.70, list=FALSE)
training &lt;- input_data[in_train,]
validation &lt;- input_data[-in_train,]
</code></pre>

<p>The following code examines the correlation coefficient. In this model, the Pearson correlation coefficient was chosen to be 0.99 (Indicating a very high level of correlation)</p>

<pre><code>M &lt;- abs(cor(training[,-53]))
diag(M) &lt;- 0
which(M &gt; 0.99,arr.ind=T)
</code></pre>

<h5>
<a name="32-pca" class="anchor" href="#32-pca"><span class="octicon octicon-link"></span></a>3.2 PCA</h5>

<p>There are variables in the data set which have a high corelation coefficient. PCA can be used to reduce the number of variables. We can set the cutoff for the cumulative percent of variance to be retained by PCA to 0.99.</p>

<pre><code>preProc=preProcess(training[,-53],method="pca",thresh=.99)
pca_train=predict(preProc,training[,-53])
pca_validation &lt;- predict(preProc, validation[-53])
</code></pre>

<p>This reduces the number of predictors in the training set from 53 to 36.</p>

<h4>
<a name="40-predictive-model" class="anchor" href="#40-predictive-model"><span class="octicon octicon-link"></span></a>4.0 Predictive Model</h4>

<p>For building the predictive model, we use the Random Forest algorithm. </p>

<pre><code>library(randomForest)
model_rf = randomForest(training$classe~., data=pca_train, ntree = 2048)
model_rf
</code></pre>

<pre><code>## 
## Call:
##  randomForest(formula = training$classe ~ ., data = pca_train,      ntree = 2048) 
##                Type of random forest: classification
##                      Number of trees: 2048
## No. of variables tried at each split: 6
## 
##         OOB estimate of  error rate: 1.99%
## Confusion matrix:
##      A    B    C    D    E class.error
## A 3898    4    1    2    1    0.002048
## B   48 2580   23    2    5    0.029345
## C    4   31 2342   17    2    0.022538
## D    3    0   86 2155    8    0.043073
## E    4   10   14    8 2489    0.014257
</code></pre>

<p>Using cross validation, the accuracy of the model can be checked. </p>

<pre><code>confusionMatrix(validation$classe, predict(model_rf, pca_validation))
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1667    5    0    0    2
##          B   20 1104   13    1    1
##          C    1   12 1006    7    0
##          D    1    1   26  932    4
##          E    0    1    8    2 1071
## 
## Overall Statistics
##                                         
##                Accuracy : 0.982         
##                  95% CI : (0.978, 0.985)
##     No Information Rate : 0.287         
##     P-Value [Acc &gt; NIR] : &lt; 2e-16       
##                                         
##                   Kappa : 0.977         
##  Mcnemar's Test P-Value : 0.000312      
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.987    0.983    0.955    0.989    0.994
## Specificity             0.998    0.993    0.996    0.994    0.998
## Pos Pred Value          0.996    0.969    0.981    0.967    0.990
## Neg Pred Value          0.995    0.996    0.990    0.998    0.999
## Prevalence              0.287    0.191    0.179    0.160    0.183
## Detection Rate          0.283    0.188    0.171    0.158    0.182
## Detection Prevalence    0.284    0.194    0.174    0.164    0.184
## Balanced Accuracy       0.993    0.988    0.976    0.991    0.996
</code></pre>

<p>The model has an accuracy of <code>0.982</code>. </p>

<h4>
<a name="50-conclusion" class="anchor" href="#50-conclusion"><span class="octicon octicon-link"></span></a>5.0 Conclusion</h4>

<p>Now that we have a model, we can use it to predict the exercise quality over the test data set. We can do this with the following code.</p>

<pre><code>test_data &lt;- test_data[, names(test_data) %in% names(input_data)]
pca_test &lt;- predict(preProc, test_data)
predicted_results &lt;- predict(model_rf, pca_test)
</code></pre>

<h4>
<a name="60-references" class="anchor" href="#60-references"><span class="octicon octicon-link"></span></a>6.0 References</h4>

<ol>
<li>Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.</li>
</ol>
        </section>

        <aside id="sidebar">
          <a href="https://github.com/ajammala/MachineLearning/zipball/master" class="button">
            <small>Download</small>
            .zip file
          </a>
          <a href="https://github.com/ajammala/MachineLearning/tarball/master" class="button">
            <small>Download</small>
            .tar.gz file
          </a>

          <p class="repo-owner"><a href="https://github.com/ajammala/MachineLearning"></a> is maintained by <a href="https://github.com/ajammala">ajammala</a>.</p>

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the Architect theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.</p>
        </aside>
      </div>
    </div>

  
  </body>
</html>