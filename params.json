{"name":"Machinelearning","tagline":"Practical Machine Learning Project","body":"### Model For Predicting Exercise Quality\r\n####1.0 Summary:\r\nIn a study, 6 test subjects were asked to perform barbell lifts correctly and incorrectly in 5 different ways while wearing accelerometers on the belt, forearms, arms and the dumbells. The data from accelerometers was recorded. The goal of this project is to build a supervised machine learning model that uses the recorded data to predict the manner in which the exercise was performed on a test data set. \r\n\r\n####2.0 Exploratory Data Analysis\r\n#####2.1 Downloading The Data\r\n\r\n```\r\nif (!file.exists(\"./pml-training.csv\")) {\r\n    download.file(\"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\", \r\n        destfile = \"./pml-training.csv\")\r\n}\r\nif (!file.exists(\"./pml-testing.csv\")) {\r\n    download.file(\"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\", \r\n        destfile = \"./pml-testing.csv\")\r\n}\r\n```\r\n\r\nReading the data into a data frame. \r\n\r\n```{r results='hide'}\r\ninput_data <- read.csv(\"./pml-training.csv\")\r\ntest_data <- read.csv(\"./pml-testing.csv\")\r\nstr(input_data)\r\n```\r\n\r\n#####2.2 Working With Missing Values\r\nNow that we have the data in a data frame, it is time to explore the columns in the data set. We know that the data set has 19622 records with 160 columns each. \r\nWe first try to look at the missing values in the data set.\r\n```\r\ndf <- colSums(is.na(input_data))\r\nplot(df, xlab = \"Variable Index\", ylab=\"Number of NAs\", type='l')\r\n```\r\n![](https://raw.githubusercontent.com/ajammala/MachineLearning/master/untitled.png)\r\nThe plot shows us that a lot of fields contain a significant number of NAs (close to 19000 NAs out of 19622 observations). We can remove these values safely because they do not affect the outcome of the experiment.  \r\n\r\n```\r\ninput_data <- read.csv(\"./pml-training.csv\", na.strings=c(\"NA\",\"\"))\r\ninput_data <- input_data[, - which(as.numeric(colSums(is.na(input_data))) > 19000)]\r\n```\r\n\r\nThe first 7 columns in the data set are `X`, `user_name`, `raw_timestamp_part_1`, `raw_timestamp_part_2`, `cvtd_timestamp`, `new_window` and `num_window`. These are static values and do not directly impact the outcome of the experiment. They can be safely removed from the data set.\r\n\r\n```\r\ninput_data <- input_data[, -c(1:7)]\r\ndim(input_data)\r\nnames(input_data)\r\n```\r\n\r\n####3.0 Feature Selection\r\nThe next step is to select the predictor variables to be used in the model.\r\n\r\n#####3.1 Corelated Predictors\r\nWe now check for Corelated predictors in the data set. If two variables are highly correlated they will impart nearly exactly the same information to the regression model. Including both variables will result in a weak model by infusing the model with noise. \r\n\r\n```\r\nlibrary(caret)\r\nset.seed(1016)\r\nin_train <- createDataPartition(input_data$classe, p=0.70, list=FALSE)\r\ntraining <- input_data[in_train,]\r\nvalidation <- input_data[-in_train,]\r\n```\r\n\r\nThe following code examines the correlation coefficient. In this model, the Pearson correlation coefficient was chosen to be 0.99 (Indicating a very high level of correlation)\r\n```\r\nM <- abs(cor(training[,-53]))\r\ndiag(M) <- 0\r\nwhich(M > 0.99,arr.ind=T)\r\n```\r\n\r\n#####3.2 PCA\r\nThere are variables in the data set which have a high corelation coefficient. PCA can be used to reduce the number of variables. We can set the cutoff for the cumulative percent of variance to be retained by PCA to 0.99.\r\n```\r\npreProc=preProcess(training[,-53],method=\"pca\",thresh=.99)\r\npca_train=predict(preProc,training[,-53])\r\npca_validation <- predict(preProc, validation[-53])\r\n```\r\n\r\nThis reduces the number of predictors in the training set from 53 to 36.\r\n\r\n####4.0 Predictive Model\r\nFor building the predictive model, we use the Random Forest algorithm. \r\n```\r\nlibrary(randomForest)\r\nmodel_rf = randomForest(training$classe~., data=pca_train, ntree = 2048)\r\nmodel_rf\r\n```\r\n\r\nUsing cross validation, the accuracy of the model can be checked. \r\n```\r\nconfusionMatrix(validation$classe, predict(model_rf, pca_validation))\r\n```\r\nThe model has an accuracy of `0.982`. \r\n\r\n####5.0 Conclusion\r\nNow that we have a model, we can use it to predict the exercise quality over the test data set. We can do this with the following code.\r\n\r\n```\r\ntest_data <- test_data[, names(test_data) %in% names(input_data)]\r\npca_test <- predict(preProc, test_data)\r\npredicted_results <- predict(model_rf, pca_test)\r\n```\r\n\r\n####6.0 References\r\n1. Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}